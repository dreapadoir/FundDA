{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55d97b6",
   "metadata": {},
   "source": [
    "<h1 style=\"color: rgb(0, 133, 202);\">Bias</h1>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 0, 0);\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c0c8e",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 133, 202);\">Exercise 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965036d1",
   "metadata": {},
   "source": [
    "The Dunning Kruger effect is a form of cognitive bias that occurs when people with low ability, expertise, or experience regarding a certain type of task or area of knowledge tend to overestimate their ability or knowledge (Wikipedia, 2022). Motta, Callaghan and Sylvester (2018) studied the effect it had on anti vaccine policy attitudes among the public. They found that one third of the group studied believed themselves to as knowledgable or more knowledgable than doctors or scientists about the causes of vaccines. This overconfidence in their expertise was highest among people with the low levels of knowledge about autism and who were more likely to endorse misinformation about the subject. This also appeared to correlate with opposition to vaccines. The issue surrounding vaccines and the Dunning Kruger effect contribution to vaccine opposition became very apparent throughout the Covid-19 pandemic. The ability to spread misinformation rapidly through social media amplified the effect and led to a widely supported opposition movement. Anti vaccine activists misinterpreted results of genuine medical trials, seized on dubious and discredited research studies and accepted as fact deliberate information. With this knowledge and the endorsement of likeminded people, they believed that they knew more than medical and public health officials and were in a better position to advise people on the safety or otherwise of vaccines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20353c9",
   "metadata": {},
   "source": [
    "A common example of cognitive bias is loss aversion or status quo bias. This occurs when a preference to avoid risk or loss is the deciding factor in a decision, even if it is not the best outcome. An example of this was given by Hartley, Lanot, & Walker (2013) in their research on the gameshow \"Who wants to be a Millionaire?\". Contestants were presented with a choice of two outcomes of equal probability. If they got it wrong, they would lose \\\\$99999. If they got it right, they would gain \\\\$400000. They had the option to not answer the question and leave with their present winnings of \\\\$100000. Contestants were more likely to leave with the money they already had rather than risk losing \\$99999. This is despite the fact that attempting the question had a better expected value than taking the winnings and leaving. The expected value is given by\n",
    "\n",
    "$$\n",
    "E(x) = \\Sigma P(x)\n",
    "$$\n",
    "\n",
    "where P is the probability of each outcome, x. In the case above we have\n",
    "\n",
    "$$\n",
    "E(x) = 0.5(-\\$99999) + 0.5(\\$400000)\\\\\n",
    "E(x) = -\\$49999.50 + \\$200000\\\\\n",
    "E(x) = \\$150000.50\n",
    "$$\n",
    "\n",
    "The expected value is significantly higher than their present winnings but the aversion to loss prevents most contestants from taking the risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1e0fe",
   "metadata": {},
   "source": [
    "The Availability Heuristic is a form of cognitive bias that influences decisions by giving extra weight to relevant examples that are more easily recalled. Common examples occur during panics following traumatic events. The probability of being the victim of a violent crime will be vastly overestimated in the wake of a high profile murder. Despite the actual probability of this happening being very low and unchanged since before the event, the ease with which an event like that can be recalled will increase the perceived risk for a time afterwards. In a study by Folkes (1988), the extent to which the name of rust remover was typical or not influenced how ineffective it was perceived to be. Some rust removers were given typical, generic names like X-Rust and Kleen and others were given atypical, distinctive names made of four random letters. Trials were run where subjects were told a rust remover worked or didn't work. The samples given were random and in a 2:1 ratio of effective to ineffective. Afterwards, the subjects were asked if a particular rust remover was effective. When a rust remover had an atypical name, it was given a higher estimate of failure than one with a typical name. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b91dab",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.researchgate.net/publication/256757335_Who_really_wants_to_be_a_millionaire_Estimates_of_risk_aversion_from_gameshow_data\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S027795361830340X?via%3Dihub\n",
    "https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect\n",
    "Folkes, V. S. (1988). The Availability Heuristic and Perceived Risk. Journal of Consumer Research, 15(1), 13â€“23. http://www.jstor.org/stable/2489168"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825f4a5",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 133, 202);\">Exercise 2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ecd9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc715a65",
   "metadata": {},
   "source": [
    "The formula for the standard deviation, $\\sigma$, is given by\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\dfrac{\\Sigma(x_{i} - \\mu)^{2}}{N}}, \\text{  where $\\mu$ is the mean and $N$ is the size of the sample.}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fd170",
   "metadata": {},
   "source": [
    "Bessel's correction involves multiplying by $\\dfrac{N}{N-1}$. It perfectly corrects the underestimation of the standard deviation when applied to the variance $\\sigma^{2}$.\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\dfrac{N}{N-1}\\Bigg(\\dfrac{\\Sigma(x_{i} - \\mu)^{2}}{N}\\Bigg)}\n",
    "$$\n",
    "\n",
    "Let $\\Delta\\sigma^{2}$ be the underestimation of the standard deviation. We can calculate it by subtracting the uncorrected variance from the variance with Bessel's correction applied.\n",
    "\n",
    "$$\n",
    "\\Delta\\sigma^{2} = \\dfrac{N}{N-1}\\Bigg(\\dfrac{\\Sigma(x_{i} - \\mu)^{2}}{N}\\Bigg) - \\dfrac{\\Sigma(x_{i} - \\mu)^{2}}{N} \\\\\n",
    "= \\dfrac{\\Sigma(x_{i} - \\mu)^{2}}{N}\\Bigg(\\dfrac{N}{N-1} - 1\\Bigg)\n",
    "$$\n",
    "\n",
    "We can see that $\\dfrac{N}{N-1} - 1$ determines the size of the underestimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41998bc7",
   "metadata": {},
   "source": [
    "If we look at the values of $\\dfrac{N}{N-1} - 1$ for different values of $N$, we can see that decrease as $N$ increases. As $N$ grows, the underestimation gets smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548f6c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5,\n",
       " 0.33333333333333326,\n",
       " 0.25,\n",
       " 0.19999999999999996,\n",
       " 0.16666666666666674,\n",
       " 0.1428571428571428,\n",
       " 0.125,\n",
       " 0.11111111111111116,\n",
       " 0.10000000000000009,\n",
       " 0.09090909090909083,\n",
       " 0.08333333333333326,\n",
       " 0.07692307692307687,\n",
       " 0.0714285714285714,\n",
       " 0.06666666666666665,\n",
       " 0.0625,\n",
       " 0.05882352941176472,\n",
       " 0.05555555555555558,\n",
       " 0.05263157894736836,\n",
       " 0.050000000000000044,\n",
       " 0.04761904761904767,\n",
       " 0.045454545454545414,\n",
       " 0.04347826086956519,\n",
       " 0.04166666666666674,\n",
       " 0.040000000000000036,\n",
       " 0.03846153846153855,\n",
       " 0.03703703703703698,\n",
       " 0.03571428571428581,\n",
       " 0.034482758620689724,\n",
       " 0.03333333333333344,\n",
       " 0.032258064516129004,\n",
       " 0.03125,\n",
       " 0.030303030303030276,\n",
       " 0.02941176470588225,\n",
       " 0.02857142857142847,\n",
       " 0.02777777777777768,\n",
       " 0.027027027027026973,\n",
       " 0.026315789473684292,\n",
       " 0.02564102564102555,\n",
       " 0.02499999999999991,\n",
       " 0.024390243902439046,\n",
       " 0.023809523809523725,\n",
       " 0.023255813953488413,\n",
       " 0.022727272727272707,\n",
       " 0.022222222222222143,\n",
       " 0.021739130434782705,\n",
       " 0.02127659574468077,\n",
       " 0.02083333333333326,\n",
       " 0.020408163265306145,\n",
       " 0.020000000000000018,\n",
       " 0.019607843137254832,\n",
       " 0.019230769230769162,\n",
       " 0.018867924528301883,\n",
       " 0.0185185185185186,\n",
       " 0.018181818181818077,\n",
       " 0.017857142857142794,\n",
       " 0.01754385964912286,\n",
       " 0.01724137931034475,\n",
       " 0.016949152542372836,\n",
       " 0.016666666666666607,\n",
       " 0.016393442622950838,\n",
       " 0.016129032258064502,\n",
       " 0.015873015873015817,\n",
       " 0.015625,\n",
       " 0.01538461538461533,\n",
       " 0.015151515151515138,\n",
       " 0.014925373134328401,\n",
       " 0.014705882352941124,\n",
       " 0.01449275362318847,\n",
       " 0.014285714285714235,\n",
       " 0.014084507042253502,\n",
       " 0.01388888888888884,\n",
       " 0.013698630136986356,\n",
       " 0.013513513513513598,\n",
       " 0.01333333333333342,\n",
       " 0.013157894736842035,\n",
       " 0.01298701298701288,\n",
       " 0.012820512820512775,\n",
       " 0.012658227848101333,\n",
       " 0.012499999999999956,\n",
       " 0.012345679012345734,\n",
       " 0.012195121951219523,\n",
       " 0.012048192771084265,\n",
       " 0.011904761904761862,\n",
       " 0.0117647058823529,\n",
       " 0.011627906976744207,\n",
       " 0.011494252873563315,\n",
       " 0.011363636363636465,\n",
       " 0.011235955056179803,\n",
       " 0.011111111111111072,\n",
       " 0.01098901098901095,\n",
       " 0.010869565217391353,\n",
       " 0.010752688172043001,\n",
       " 0.010638297872340496,\n",
       " 0.010526315789473717,\n",
       " 0.01041666666666674,\n",
       " 0.010309278350515427,\n",
       " 0.010204081632652962]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(N):\n",
    "    factor = N/(N-1) - 1\n",
    "    return factor\n",
    "\n",
    "list = []\n",
    "for i in range(3, 100):\n",
    "    list.append(test(i))\n",
    "\n",
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a94d4b4",
   "metadata": {},
   "source": [
    "If we plot the values for $\\Delta\\sigma^{2}$ we can see that correction factor is significantly higher for smaller values of $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3c0d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1886e8bf640>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971e54f",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 133, 202);\">Guessing Game function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ae7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2(L):\n",
    "    doubleLength = len(L) * 2  #twice the number of elements in the list\n",
    "    constant = 17  #0b10001 means 10001 in binary, 17 in decimal. \n",
    "    listDifferences = []\n",
    "    for i in L:\n",
    "        a = i - 2  #0b1 is 2 in binary\n",
    "        listDifferences.append(a)\n",
    "    sumDifferences = sum(listDifferences) #sum([i - 0b10 for i in L]) is the sum of each number in the list minus 2\n",
    "    comparisons = []  \n",
    "    for i in range(len(L)-1):\n",
    "        if L[-i-1] >= L[-i-2]: #L[-i-1] is equivalent to L[::-1][i] and L[-i-2] is equivalent to L[::-1][i+0b1] (0b1 is 1 in binary)\n",
    "            comparisons.append(1)\n",
    "        else:\n",
    "            comparisons.append(0)\n",
    "    sumComparisons = sum(comparisons)\n",
    "    \n",
    "    #the test is checking two things, 1. that 17 is greater than or equal to the sum of twice the length of the set\n",
    "    #and the sum of the results of every element in the set minus two, 2. that each element except the first is greater than\n",
    "    #or equal to the element preceding it\n",
    "    return True if constant >= (doubleLength + sumDifferences) and sumComparisons == len(L) - 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f508e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
